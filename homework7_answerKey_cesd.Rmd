---
title: "N741 Spring 2018 - Homework 7 - ANSWER KEY - CESD"
author: "Melinda Higgins"
date: "April 4, 2018"
output:
  html_document: default
  pdf_document: default
subtitle: Homework 7 - DUE WED April 11, 2018
---

# RMD with code for `cesd`

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(error = FALSE)
```

## Homework 7

### Background and Information on HELP Dataset

For homework 7, you will be working with the **HELP** (Health Evaluation and Linkage to Primary Care) Dataset. See complete details posted in Homework 6.

### Variables for Homework 7

For Homework 7, you will focus on these variables from the HELP dataset:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
helpdata <- haven::read_spss("helpmkh.sav")

h1 <- helpdata %>%
  select(age, female, pss_fr, homeless, 
         pcs, mcs, cesd)

# add dichotomous variable
# to indicate depression for
# people with CESD scores >= 16
# and people with mcs scores < 45

h1 <- h1 %>%
  mutate(cesd_gte16 = cesd >= 16) %>%
  mutate(mcs_lt45 = mcs < 45)

# change cesd_gte16 and mcs_lt45 LOGIC variable type
# to numeric coded 1=TRUE and 0=FALSE

h1$cesd_gte16 <- as.numeric(h1$cesd_gte16)
h1$mcs_lt45 <- as.numeric(h1$mcs_lt45)

# add a label for these 2 new variables
attributes(h1$cesd_gte16)$label <- "Indicator of Depression"
attributes(h1$mcs_lt45)$label <- "Indicator of Poor Mental Health"

# create a function to get the label
# label output from the attributes() function
getlabel <- function(x) attributes(x)$label
# getlabel(sub1$age)

library(purrr)
ldf <- purrr::map_df(h1, getlabel) # this is a 1x15 tibble data.frame
# t(ldf) # transpose for easier reading to a 15x1 single column list

# using knitr to get a table of these
# variable names for Rmarkdown
library(knitr)
knitr::kable(t(ldf),
             col.names = c("Variable Label"),
             caption="Use these variables from HELP dataset for Homework 07")

```

## Homework 7 Assignment

**SETUP** Download and run the "loadHELP.R" `R` script (included in this Github repo [https://github.com/melindahiggins2000/N741Spring2018_Homework7](https://github.com/melindahiggins2000/N741Spring2018_Homework7)) to read in the HELP Dataset "helpmkh.sav". This script also pulls out the variables you need and creates the dichotomous variable for depression `cesd_gte16` **AND** a dichotomous variable to indicate poor mental health (`mcs_lt45`).

After running this R script, you will have a data frame called `h1` you can use to do the rest of your analyses. You can also copy this code into your first R markdown code chunk to get you started on Homework 7 - or begin with this R markdown file for Homework 7.

For Homework 7, the code is provided here for the regression tree and conditional tree and random forest models looking at depression as given by the continuous measure `cesd` and the dichotomous indicator of depression `cesd_gte16`.

You can then use this code and adapt it to run through the models again looking at the mental health composite score (`mcs`) in these subjects and the dichomotous indicator or poor mental health for people with `mcs` scores < 45, which is the variable `mcs_lt45`. 

### Packages needed for Homework 7

* `rpart`
* `partykit`
* `party`
* `tidyverse`
* `reshape2`
* `randomForestSRC`
* `ggRandomForests`

```{r}
library(rpart)
library(partykit)
library(reshape2)
library(party)
library(tidyverse)
library(randomForestSRC)
library(ggRandomForests)
```

### Regression Tree for CESD

Using the `rpart` package and the steps we demonstrated in class, the code below:

* fits a regression tree to the `cesd` based on only the `mcs` scores from the `h1` dataset;
* displays the results
* plots the cross-validated results
* provides a summary of the model fit
* and plots the regression tree

```{r}
# fit a regression tree model to the cesd as the outcome
# and using the mcs as the only predictor
fitcesd <- rpart::rpart(cesd ~ mcs, data = h1)
rpart::printcp(fitcesd) # Display the results
rpart::plotcp(fitcesd) # Visualize cross-validation results
summary(fitcesd) # Detailed summary of fit

# plot tree
plot(fitcesd, uniform = TRUE, compress = FALSE)
text(fitcesd, use.n = TRUE, all = TRUE, cex = 0.5)
```

### Matrix Scatterplot of Other Variables with CESD

We can use the `reshape2` package to basically stack all of the other variables on top of one another and align them with the `cesd` variable and then use this "melted" dataset with the `facet_wrap` option with `ggplot()` to basically get a matrix of scatterplots showing how all of the other variables are associated with the `cesd`.

I also first remove the variables I don't need for this next step and create the dataset `h1a`.

```{r}
# all vars except the dichotomous cesd_gte16 and mcs_lt45
h1a <- h1[,1:7]

# Melt the other variables down and link to cesd
h1m <- reshape2::melt(h1a, id.vars = "cesd")

# Plot panels for each covariate
ggplot(h1m, aes(x=cesd, y=value)) +
  geom_point(alpha=0.4)+
  scale_color_brewer(palette="Set2")+
  facet_wrap(~variable, scales="free_y", ncol=3)
```

### Regression Tree for CESD with the rest of the variables

Now let's see what happens when we include the rest of the variables. A "shorthand" notation used in R that can be handy is to simply put in a period "." indicating use the rest of the variables in the model.

So, the line of code

```
fitall <- rpart::rpart(cesd ~ ., data = h1a)
```

basically says to fit a model for `cesd` from the rest of the variables in the dataset `h1a` which includes: 

* `age`
* `female`
* `pss_fr`
* `homeless`
* `pcs`
* `mcs`

So the period "." in the model formula `cesd ~ .` part of the code above indicates that we're going to put `age`, `female`, `pss_fr`, `homeless`, `pcs`, and `mcs` into the model as predictors.

But the equivalent way to define this model where you list each variable you want in the model is to use the plus `+` symbol between each variable - so you could also write this code:

```
fitall <- rpart::rpart(cesd ~ age + female + pss_fr + 
                              homeless + pcs + mcs, 
                              data = h1a)
```

So, let's see what the regression tree for CESD looks like if we try all of these other variables as predictors in the model.

```{r}
# fit a regression tree with all vars
fitall <- rpart::rpart(cesd ~ ., data = h1a)

# equivalent code statement without the shorthand
# using the period for the "rest of the variables"
# this time each variable to be included is listed
# individually putting a plus + in between each 
# variable added to the model

fitall <- rpart::rpart(cesd ~ age + female + pss_fr + 
                              homeless + pcs + mcs, 
                              data = h1a)

# Now let's look at fitall
rpart::printcp(fitall) # Display the results
rpart::plotcp(fitall) # Visualize cross-validation results
summary(fitall) # Detailed summary of fit

plot(fitall, uniform = TRUE, compress = FALSE, main = "Regression Tree for CESD Scores from HELP(h1) Data")
text(fitall, use.n = TRUE, all = TRUE, cex = 0.5)
```

### Regression Tree for CESD Using the `party` package approach

The `party` package has better graphics and fits a "conditional" regression tree using the `ctree()` function. Here is the model approach for the `cesd` using the rest of the variables in the dataset `h1a`.

```{r}
fitallp <- party::ctree(cesd ~ ., data = h1a)
plot(fitallp, main = "Conditional Inference Tree for CESD")
```

### Logistic Regression of CESD => 16 

When the outcome is dichotomous or is a categorical outcome, you can fit a "decision tree" or "classification tree". One way you've already learned last week is fitting a logistic regression model. In fact, logistic regression is a supervised classification modeling approach. Let's .ee what this looks like for predicting depression (indicated by `cesd_gte16` for people with CESD scores => 16). Pay attention to which variables are significant in the resulting logistic regression model.

```{r}
# begin with a logistic regression - depressed or not
glm1 <- glm(cesd_gte16 ~ age + female + pss_fr + homeless + 
              pcs + mcs, data = h1)
summary(glm1)
```

### Fit a Classification Tree for CESD => 16

We can use the `rpart` package again to fit a classification tree to the depression indicator `cesd_gte16`.

```{r}
fitk <- rpart::rpart(cesd_gte16 ~ age + female + pss_fr + 
                       homeless + pcs + mcs, 
                     method = "class", data = h1)
class(fitk)
# Display the results
rpart::printcp(fitk)
#Visualize the cross-validation results 
rpart::plotcp(fitk)
# Get a detailed summary of the splits
summary(fitk)
# Plot the tree
plot(fitk, uniform = TRUE, 
     main = "Classification Tree for CESD => 16")
text(fitk, use.n = TRUE, all = TRUE, cex = 0.8)
```

### Fit a Conditional Classification Tree for CESD => 16 

Using the `party` package, we can fit a conditional classification tree using the `ctree()` function. Let's do one for the indicator of depression `cesd_gte16` given the other variables in the `h1` dataset: `age`, `female`, `pss_fr`, `homeless`, `pcs`, `mcs`. 

```{r}
# look at cesd_gte16 with ctree from party
fitallpk <- party::ctree(cesd_gte16 ~ age + female + pss_fr + 
                           homeless + pcs + mcs, data = h1)
class(fitallpk)
plot(fitallpk, main = "Conditional Inference Tree for CESD => 16")
```

### Recursive Partitioning of Classification Tree for CESD => 16

Here is the code doing the recursive partitioning of CESD => 16 on `age`, `female`, `pss_fr`, `homeless`, `pcs`, `mcs`. We're also using the `partykit` package to get prettier graphics for this classification tree.

```{r}
# Recursive partitioning of CESD => 16 on age, 
# female, pss_fr, homeless, pcs, mcs
whoIsDepressed <- rpart::rpart(cesd_gte16 ~ age + female + 
                                 pss_fr + homeless + pcs + mcs,
                               data = h1, 
                               control = rpart.control(cp = 0.001,
                                                       minbucket = 20))

whoIsDepressed

library(partykit)
# Plot the tree
plot(partykit::as.party(whoIsDepressed))
```

### Scatterplot of recursive partitions for CESD => 16 for MCS and PCS

The code below creates a scatterplot of `pcs` and `mcs` where the points are colored by the indication of depression `cesd_gte16`. The lines have been inserted showing the dividing lines that best separate subjects with depression (CESD => 16) from those without depression (CESD < 16).

```{r}
# EXTRA CREDIT
# Graph as partition
# using the break points shown from the
# conditional tree
ggplot(data = h1, aes(x = mcs, y = pcs)) +
  geom_count(aes(color = cesd_gte16), alpha = 0.5) +
  geom_vline(xintercept = 50.024) +
  geom_vline(xintercept = 41.164) +
  geom_vline(xintercept = 37.054) +
  geom_segment(x = 37.054, xend = 0, y = 58.164, yend = 58.164) +
  annotate("rect", xmin = 0, xmax = 100, ymin = 0, ymax = 100, fill = "blue", alpha = 0.1) +
  ggtitle("CESD => 16 Partitioned By MCS and PCS - Dark Circles Not Depressed")
```

### Random Forest Model for CESD

Now let's use a Random Forest approach for modeling the CESD by the other variables in the dataset: 

* `age`
* `female`
* `pss_fr`
* `homeless`
* `pcs`
* `mcs`

And using the code below, we'll explore how well the model converges and how well it does predicting CESD scores.

```{r}
h1 <- as.data.frame(h1)
set.seed(131)
# Random Forest for the h1 dataset
fitallrf <- randomForestSRC::rfsrc(cesd ~ age + female + 
                                     pss_fr + homeless + pcs + mcs, 
                                   data = h1, ntree = 100, 
                                   tree.err=TRUE)
# view the results
fitallrf
gg_e <- ggRandomForests::gg_error(fitallrf)
plot(gg_e)

# Plot the predicted cesd values
plot(ggRandomForests::gg_rfsrc(fitallrf), alpha = 0.5)

# Plot the VIMP rankins of independent variables
plot(ggRandomForests::gg_vimp(fitallrf))

# Select the variables
varsel_cesd <- randomForestSRC::var.select(fitallrf)
glimpse(varsel_cesd)

# Save the gg_minimal_depth object for later use
gg_md <- ggRandomForests::gg_minimal_depth(varsel_cesd)
# Plot the object
plot(gg_md)

# Plot minimal depth v VIMP
gg_mdVIMP <- ggRandomForests::gg_minimal_vimp(gg_md)
plot(gg_mdVIMP)
```

### Create Plots of How Well Each Variable Predicts CESD

Using the code below, we can see how well each variable predicts CESD scores.

```{r}
#Create the variable dependence object from the random forest
gg_v <- ggRandomForests::gg_variable(fitallrf)

# Use the top ranked minimal depth variables only, plotted in minimal depth rank order
xvar <- gg_md$topvars

# Plot the variable list in a single panel plot
plot(gg_v, xvar = xvar, panel = TRUE, alpha = 0.4) +
  labs(y="Predicted CESD reading", x="")
```

---

**Use R markdown to complete your homework and show all of your code and output in your final report - Turn in a PDF of your report to Canvas. Include a link to your Github repo for Homework 7**

---


